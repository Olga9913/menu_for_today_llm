# menu_for_today_llm

## Описание
Генератор по созданию рецептов по запросам пользователей

Проект использует LLM (Mistral 7B) и RAG для подбора рецептов на основе:
- Диетических ограничений (вегетарианское, безглютеновое и т.д.).
- Доступных ингредиентов.
- Времени приготовления.

## Запуск проекта

1. **Установка окружения**:
   ```bash
   python3 -m venv healthyai_venv
   source healthyai_venv/bin/activate
   pip install -r requirements.txt
   python3 -m spacy download ru_core_news_sm

2. **Запуск Ollama (Mistral 7B)**:
    ```bash
    curl -fsSL https://ollama.com/install.sh | sh
    ollama pull mistral
    ollama serve &
3. **Подготовка данных**:

- Разархивируйте файлы в data/recipes_*.zip.

- Запустите создание данных с сайта povar.ru:

    ```bash
    python3 scripts/create_pickle_files.py

4. **Запуск RAG, графа знаний, векторного хранилища и токенизатора**:

    ```bash
    python3 scripts/pipeline.py 

5. **Архитектура и компоненты**:

    - Chroma: векторная база данных для хранения и поиска эмбеддингов рецептов.
    - Embedding Model (ai-forever/sbert_large_nlu_ru): модель для преобразования текстов (рецептов, ингредиентов, запросов) в векторные представления.
    - Ollama (Mistral 7B): локально развернутая языковая модель для обработки запросов и генерации ответов.
    - NetworkX: библиотека для построения графа знаний, связывающего рецепты и ингредиенты.
    - LangChain: фреймворк для оркестрации взаимодействия между LLM, векторной базой и графом знаний.

Эти компоненты работают вместе, чтобы обработать пользовательский запрос, найти подходящие рецепты и сгенерировать понятный и точный ответ.

6. **Схема взаимодействие компонентов**:

    - Ввод запроса. Пользователь отправляет запрос, например, "Традиционный французский луковый суп".
    - Запрос передается в LangChain, который координирует дальнейшую обработку.
    - Генерация эмбеддинга запроса: текст запроса преобразуется в векторное представление с помощью модели ai-forever/sbert_large_nlu_ru.
    - Поиск в Chroma: эмбеддинг запроса отправляется в Chroma, где выполняется поиск ближайших по косинусному сходству эмбеддингов рецептов. Chroma возвращает список релевантных рецептов
    - Использование графа знаний (NetworkX): если запрос включает конкретные ингредиенты (например, "лук", "грибы"), LangChain обращается к графу знаний, построенному с помощью NetworkX.
    - Граф содержит узлы (рецепты и ингредиенты) и ребра (связи между рецептами и используемыми ингредиентами). Это позволяет уточнить поиск, найдя рецепты, соответствующие указанным ингредиентам.
    - LangChain объединяет данные из Chroma и NetworkX и передает их в Mistral 7B через Ollama.
    - Модель Mistral 7B генерирует структурированный ответ, включающий название рецепта, список ингредиентов, инструкции.
    - Возврат ответа: langChain форматирует сгенерированный ответ и возвращает его пользователю в удобном виде (например, текст с четкими шагами приготовления).


### **Особенности проекта**  
 
- **Персонализация**:  
  - Подбор рецептов на основе **калорийности, баланса БЖУ и времени приготовления**.  
  - Учет диетических ограничений: **веганские, безглютеновые, низкоуглеводные** варианты.  
- **ЗОЖ-метрики**:  
  - Оценка рецептов по **полезности** (например, низкое содержание сахара, высокое содержание белка).  
  - Возможность **замены ингредиентов** на более здоровые альтернативы.  

#### **Технические улучшения**  
- Использование **Mistral 7B** через Ollama для быстрого развертывания без обучения.  
- Оптимизированная обработка данных (**30–60 минут** для 1000+ рецептов).  
- Векторный поиск (Chroma DB) + граф знаний для точных рекомендаций.  

### **Результаты и выводы**
- Бенчмарк - BERTScore, потому что он хорошо подходит для задач генерации текста, так как учитывает семантическое сходство, а не только точное совпадение слов (в отличие от BLEU или ROUGE).

```bash
  0.66 для RAG, 0.69 для графа знаний, 0.74 для LLM + RAG
```
Метрики BERTScore (0.66 для RAG, 0.69 для графа знаний, 0.74 для LLM + RAG) показывают, что комбинация всех компонентов дает наилучший результат:

- RAG (Chroma): эффективен для поиска по тексту, но не учитывает сложные связи между ингредиентами.
- Граф знаний (NetworkX): улучшает точность за счет анализа ингредиентов, но ограничен структурой графа. Дает высокие precision и recall, но его F1 ниже, чем у LLM + RAG, что может говорить о его избыточности или недостаточной адаптивности.
- LLM + RAG: комбинирует преимущества поиска и генерации, добавляя контекст и замены ингредиентов, что повышает семантическую точность.

```bash
{'precision': [0.7065095901489258, 0.9999998807907104],
 'recall': [0.7310776710510254, 0.9999998807907104],
 'f1': [0.7185837030410767, 0.9999998807907104],
 'hashcode': 'bert-base-multilingual-cased_L9_no-idf_version=0.3.12(hug_trans=4.51.3)'}
 ```
- Precision:

0.706 (RAG) vs. 0.999 (граф знаний)

Граф знаний дает почти идеальную точность, что может означать, что он реже генерирует ложные срабатывания, но, возможно, за счет меньшего охвата.

RAG и LLM + RAG имеют более сбалансированную точность, что говорит о лучшем компромиссе между релевантностью и полнотой.

- Recall:

0.731 (RAG) vs. 0.999 (граф знаний)

Граф знаний почти идеален по полноте, но это может быть связано с его жесткой структурой (например, он возвращает все возможные связи, даже если они не всегда полезны).

RAG и LLM + RAG имеют более реалистичный recall, что может означать, что они лучше фильтруют информацию.

- F1:

LLM + RAG (0.74), Граф знаний (0.69), RAG (0.66)

Комбинация LLM и RAG улучшает качество ответов, вероятно, за счет лучшей интерпретации запросов (LLM), более релевантного поиска (RAG), граф знаний хорош, но, возможно, менее гибок в сложных запросах.


### **Планы по улучшению**

- Интерактивный выбор порций: пересчет ингредиентов для разного количества порций.
- Оптимизация времени приготовления: добавление фильтра в Chroma по времени, чтобы предлагать быстрые рецепты.